{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "TASK-2 : **Chat with Website Using RAG Pipeline**"
      ],
      "metadata": {
        "id": "YibNjKkHlFyL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Overview**\n",
        "\n",
        "The goal is to implement a Retrieval-Augmented Generation (RAG) pipeline that allows users to\n",
        "interact with structured and unstructured data extracted from websites. The system will crawl,\n",
        "scrape, and store website content, convert it into embeddings, and store it in a vector database.\n",
        "Users can query the system for information and receive accurate, context-rich responses\n",
        "generated by a selected LLM.\n"
      ],
      "metadata": {
        "id": "IWjQnamAlL8q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Installing the necessary packages ***"
      ],
      "metadata": {
        "id": "Gkq0dVmtlgoX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests\n",
        "!pip install beautifulsoup4\n",
        "!pip install sentence-transformers\n",
        "!pip install torch torchvision torchaudio\n",
        "!pip install transformers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-DM_HbblvMK",
        "outputId": "5b214270-60cf-4d31-ee59-455135ed939c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.2.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.46.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.6)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.5.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.26.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (11.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing the packages\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import sqlite3\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "#Defining the models\n",
        "# Constants\n",
        "EMBEDDING_MODEL_NAME = 'all-MiniLM-L6-v2'  # SentenceTransformer for embeddings\n",
        "LLM_MODEL_NAME = 'bigscience/bloom-560m'  # Hugging Face model for response generation\n",
        "DATABASE_NAME = 'embeddings.db'\n",
        "\n",
        "# Step 1: Crawl and Scrape Website\n",
        "def crawl_and_scrape(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    texts = [p.get_text().strip() for p in soup.find_all('p') if p.get_text().strip()]\n",
        "    return texts\n",
        "\n",
        "#Explanation:\n",
        "#Purpose: Scrapes the content of a given website URL, specifically extracting paragraphs of text.\n",
        "#Steps:\n",
        "#1 BeautifulSoup(response.text, 'html.parser') parses the HTML content.\n",
        "#2.Filters out empty or whitespace-only paragraphs using a list comprehension.\n",
        "#3.Output: A list of non-empty text strings from the <p> tags of the webpage.\n",
        "\n",
        "\n",
        "\n",
        "# Step 2: Generate Embeddings\n",
        "def generate_embeddings(texts, model):\n",
        "    return model.encode(texts, convert_to_tensor=True)\n",
        "\n",
        "#Explanation: Purpose: Converts the list of text strings into high-dimensional vectors (embeddings) using a pre-trained SentenceTransformer model.\n",
        "#Steps:\n",
        "#1.model.encode() generates embeddings for each text.\n",
        "#2.convert_to_tensor=True ensures the output is a PyTorch tensor.\n",
        "#3.Output: A tensor containing embeddings for the input texts. Each embedding represents a text in vector space.\n",
        "\n",
        "\n",
        "# Step 3: Store Embeddings in SQLite Database\n",
        "def setup_database():\n",
        "    conn = sqlite3.connect(DATABASE_NAME)\n",
        "    c = conn.cursor()\n",
        "    c.execute('''CREATE TABLE IF NOT EXISTS embeddings (\n",
        "                    id INTEGER PRIMARY KEY,\n",
        "                    text TEXT,\n",
        "                    embedding BLOB\n",
        "                )''')\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "#Explanation:\n",
        "#Purpose: Prepares a SQLite database to store text and their embeddings persistently.\n",
        "\n",
        "#Steps:\n",
        "#1.Connects to (or creates) a SQLite database file named DATABASE_NAME.\n",
        "#2.Creates a table embeddings with columns:\n",
        "#3.embedding: The binary representation of the embedding.\n",
        "#4.Ensures the table exists using CREATE TABLE IF NOT EXISTS.\n",
        "#5.Output: A database setup ready to store text and embeddings.\n",
        "\n",
        "\n",
        "\n",
        "def store_embeddings(texts, embeddings):\n",
        "    conn = sqlite3.connect(DATABASE_NAME)\n",
        "    c = conn.cursor()\n",
        "    for text, embedding in zip(texts, embeddings):\n",
        "        c.execute(\"INSERT INTO embeddings (text, embedding) VALUES (?, ?)\", (text, embedding.numpy().tobytes()))\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "#Explanation:\n",
        "#Purpose: Stores text and their embeddings into the SQLite database.\n",
        "#Steps:\n",
        "#1.Opens a connection to the database.\n",
        "#2.Iterates over texts and their corresponding embeddings.\n",
        "#3.Converts embeddings to binary format (tobytes()) for storage in the database.\n",
        "#4.Inserts each text and its embedding into the embeddings table.\n",
        "#5.Output: The database is populated with texts and their embeddings\n",
        "\n",
        "# Step 4: Query Handling and Cosine Similarity\n",
        "def retrieve_relevant_chunks(query, model):\n",
        "    query_embedding = model.encode(query, convert_to_tensor=True)\n",
        "    conn = sqlite3.connect(DATABASE_NAME)\n",
        "    c = conn.cursor()\n",
        "    c.execute(\"SELECT text, embedding FROM embeddings\")\n",
        "    rows = c.fetchall()\n",
        "    conn.close()\n",
        "\n",
        "    texts = []\n",
        "    similarities = []\n",
        "    for text, embedding_blob in rows:\n",
        "        embedding = torch.tensor(torch.frombuffer(embedding_blob,dtype=torch.float32))\n",
        "        similarity = util.pytorch_cos_sim(query_embedding, embedding)[0][0].item()\n",
        "        texts.append(text)\n",
        "        similarities.append(similarity)\n",
        "\n",
        "    # Sort by similarity and return top chunks\n",
        "    sorted_texts = [text for _, text in sorted(zip(similarities, texts), reverse=True)]\n",
        "    return sorted_texts[:5]  # Return top 5 chunks\n",
        "\n",
        "#Explanation: Purpose: Finds the most relevant pieces of text from the database for a given query using cosine similarity.\n",
        "#Steps:\n",
        "#1.Encodes the query into an embedding.\n",
        "#2.Fetches all stored texts and embeddings from the database.\n",
        "#3.Decodes binary embeddings using torch.frombuffer and calculates the cosine similarity with the query embedding using util.pytorch_cos_sim.\n",
        "#4.Sorts texts based on similarity scores in descending order.\n",
        "#5.Returns the top 5 most relevant texts.\n",
        "#Output: A list of the top 5 relevant text chunks based on the query.\n",
        "\n",
        "\n",
        "\n",
        "# Step 5: Response Generation\n",
        "def generate_response(retrieved_chunks, query, model, tokenizer):\n",
        "    prompt = f\"Context: {retrieved_chunks}\\n\\nQuestion: {query}\\n\\nAnswer:\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    # Increase max_length or better yet, use max_new_tokens\n",
        "    outputs = model.generate(**inputs, max_new_tokens=200, num_beams=3, no_repeat_ngram_size=2)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "#Explanation: Purpose: Uses a language model to generate a response to the query based on the retrieved relevant texts.\n",
        "#Steps:\n",
        "#1.Constructs a prompt combining the retrieved texts (retrieved_chunks) and the query.\n",
        "#2.Tokenizes the prompt using tokenizer.\n",
        "#3.Generates a response using the language model (model.generate()), with parameters:\n",
        "#4.Decodes the generated tokens into a readable string.\n",
        "#Output: A string representing the generated response.\n",
        "\n",
        "\n",
        "# Main Workflow\n",
        "def main():\n",
        "    # Models\n",
        "    embedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME)\n",
        "    llm_model = AutoModelForCausalLM.from_pretrained(LLM_MODEL_NAME)\n",
        "    llm_tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL_NAME)\n",
        "\n",
        "    # Setup database\n",
        "    setup_database()\n",
        "\n",
        "    # Crawl and scrape websites\n",
        "    urls = [\"https://www.uchicago.edu/\", \"https://www.washington.edu/\"]\n",
        "    for url in urls:\n",
        "        print(f\"Crawling and scraping: {url}\")\n",
        "        texts = crawl_and_scrape(url)\n",
        "        print(f\"Found {len(texts)} texts. Generating embeddings...\")\n",
        "\n",
        "        embeddings = generate_embeddings(texts, embedding_model)\n",
        "        store_embeddings(texts, embeddings)\n",
        "        print(f\"Stored {len(texts)} embeddings.\")\n",
        "\n",
        "    # Query and respond\n",
        "    user_query = input(\"Enter your question: \")\n",
        "    print(\"Retrieving relevant chunks...\")\n",
        "    relevant_chunks = retrieve_relevant_chunks(user_query, embedding_model)\n",
        "\n",
        "    print(\"Generating response...\")\n",
        "    response = generate_response(relevant_chunks, user_query, llm_model, llm_tokenizer)\n",
        "    print(f\"Response:\\n{response}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "#Explanation:\n",
        "#1.Loads the embedding and language models.\n",
        "#2.Sets up the database.\n",
        "#3.Scrapes and stores website content into the database.\n",
        "#4.Accepts a query from the user.\n",
        "#5.Retrieves relevant texts for the query.\n",
        "#6.Generates a response using the LLM."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E36VPN_suaA4",
        "outputId": "7df840b6-cf16-4a24-f70a-719e5a31983b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Crawling and scraping: https://www.uchicago.edu/\n",
            "Found 1 texts. Generating embeddings...\n",
            "Stored 1 embeddings.\n",
            "Crawling and scraping: https://www.washington.edu/\n",
            "Found 15 texts. Generating embeddings...\n",
            "Stored 15 embeddings.\n",
            "Enter your question: What major achievements have come from Stanford University's research programs?\n",
            "Retrieving relevant chunks...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-03bb9598d3ab>:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  embedding = torch.tensor(torch.frombuffer(embedding_blob,dtype=torch.float32))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating response...\n",
            "Response:\n",
            "Context: ['David Baker, professor of biochemistry at the UW School of Medicine in Seattle, received the 2024 Nobel Prize in Chemistry. Nobel Week wove stately traditions with imaginative recognitions.', 'David Baker, professor of biochemistry at the UW School of Medicine in Seattle, received the 2024 Nobel Prize in Chemistry. Nobel Week wove stately traditions with imaginative recognitions.', 'David Baker, professor of biochemistry at the UW School of Medicine in Seattle, received the 2024 Nobel Prize in Chemistry. Nobel Week wove stately traditions with imaginative recognitions.', 'David Baker, professor of biochemistry at the UW School of Medicine in Seattle, received the 2024 Nobel Prize in Chemistry. Nobel Week wove stately traditions with imaginative recognitions.', 'David Baker, professor of biochemistry at the UW School of Medicine in Seattle, received the 2024 Nobel Prize in Chemistry. Nobel Week wove stately traditions with imaginative recognitions.']\n",
            "\n",
            "Question: What major achievements have come from Stanford University's research programs?\n",
            "\n",
            "Answer: The Stanford Research Institute (SRI) is the largest research organization in the United States. The SRI is responsible for the development and implementation of the National Institutes of Health's (NIH) National Center for Biotechnology Information (NCBI) and National Science Foundation (NSF) funded research projects. In addition to the NIH and NSF funded projects, the Institute also has a number of non-funded projects that are funded by other organizations, such as the U.S. Department of Energy (DOE), the European Union (EU), and the World Health Organization (WHO). The Institute is also a member of several international research consortia, including the International Union of Pure and Applied Chemistry (IUPAC), International Society for Chemical Engineering (ISCE), The International Association of Chemical Engineers (IAE); the American Chemical Society (ACS), American Society of Testing and Materials (ASTM), National Academy of Sciences (NAS), European Chemical Societies (\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output:\n",
        "\n",
        "Query:\n",
        "\n",
        "Enter your question: What major achievements have come from Stanford University's research programs?\n",
        "\n",
        "Answer:\n",
        "\n",
        "The Stanford Research Institute (SRI) is the largest research organization in the United States. The SRI is responsible for the development and implementation of the National Institutes of Health's (NIH) National Center for Biotechnology Information (NCBI) and National Science Foundation (NSF) funded research projects. In addition to the NIH and NSF funded projects, the Institute also has a number of non-funded projects that are funded by other organizations, such as the U.S. Department of Energy (DOE), the European Union (EU), and the World Health Organization (WHO). The Institute is also a member of several international research consortia, including the International Union of Pure and Applied Chemistry (IUPAC), International Society for Chemical Engineering (ISCE), The International Association of Chemical Engineers (IAE); the American Chemical Society (ACS), American Society of Testing and Materials (ASTM), National Academy of Sciences (NAS).\n",
        "\n",
        "This is the answer it generates when we give the query to the model\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aEzCRN9HoxZE"
      }
    }
  ]
}